{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from hard_test import parallel_bronKerbosch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pi calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_point_inside_unit_circle(_):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_local(num_samples):\n",
    "    count = 0\n",
    "    for i in range(num_samples):\n",
    "        if is_point_inside_unit_circle(i):\n",
    "            count = count + 1\n",
    "\n",
    "    pi_estimate = 4 * count / num_samples\n",
    "    print(\"Approximate value of Pi is:\", pi_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_cluster(num_samples, sc):\n",
    "    \n",
    "    # Parallelize the process\n",
    "    count = sc.parallelize(range(0, num_samples)) \\\n",
    "              .filter(is_point_inside_unit_circle) \\\n",
    "              .count()\n",
    "\n",
    "    # Calculate and print the approximate value of Pi\n",
    "    pi_estimate = 4 * count / num_samples\n",
    "    print(\"Approximate value of Pi is:\", pi_estimate)\n",
    "\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/03 19:39:15 WARN Utils: Your hostname, coartix-ubuntu resolves to a loopback address: 127.0.1.1; using 10.41.175.200 instead (on interface wlp0s20f3)\n",
      "23/12/03 19:39:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/03 19:39:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "num_samples = int(1e9)\n",
    "conf = SparkConf().setAppName(\"Benchmark\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate value of Pi is: 3.141511264\n",
      "Time elapsed:  244.77220606803894\n"
     ]
    }
   ],
   "source": [
    "# benchmark local\n",
    "start_time = time.time()\n",
    "main_local(num_samples)\n",
    "end_time = time.time()\n",
    "print(\"Time elapsed: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate value of Pi is: 3.141539956\n",
      "Time elapsed:  98.69377899169922\n"
     ]
    }
   ],
   "source": [
    "# benchmark spark local\n",
    "start_time = time.time()\n",
    "main_cluster(num_samples, sc)\n",
    "end_time = time.time()\n",
    "print(\"Time elapsed: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark spark cluster\n",
    "!/opt/spark/bin/spark-submit --master spark://10.41.175.200:7077 --deploy-mode client --num-executors 4 --executor-cores 4 simple_test.py > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Approximate value of Pi is: 3.14165072\n",
      "INFO:root:Time elapsed: 15.578842401504517\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "INFO:root:Approximate value of Pi is: 3.141582504\n",
      "INFO:root:Time elapsed: 124.8446033000946\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('spark_app.log', 'r') as log_file:\n",
    "    log_content = log_file.read()\n",
    "    print(log_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BronKerbosch algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bronKerbosch1(R, P, X, G, res = []):\n",
    "    \"\"\"Algorithme de Bron-Kerbosch pour trouver les cliques d'un graphe.\"\"\"\n",
    "    if len(P) == 0 and len(X) == 0:\n",
    "        res.append(R)\n",
    "    for v in P.copy():\n",
    "        bronKerbosch1(R.union({v}), P.intersection(set(G.neighbors(v))), X.intersection(set(G.neighbors(v))), G, res=res)\n",
    "        P.remove(v)\n",
    "        X.add(v)\n",
    "    return res\n",
    "\n",
    "def bronKerbosch2(R, P, X, G):\n",
    "    \"\"\"Algorithme de Bron-Kerbosch pivot pour trouver les cliques d'un graphe.\"\"\"\n",
    "    if len(P) == 0 and len(X) == 0:\n",
    "        print(R)\n",
    "    else:\n",
    "        u = P.union(X).pop()\n",
    "        for v in P.difference(set(G.neighbors(u))):\n",
    "            bronKerbosch2(R.union({v}), P.intersection(set(G.neighbors(v))), X.intersection(set(G.neighbors(v))), G)\n",
    "            P.remove(v)\n",
    "            X.add(v)\n",
    "\n",
    "def bronKerbosch3(G):\n",
    "    \"\"\"Algorithme de Bron-Kerbosch pivot et dégénérescence pour trouver les cliques d'un graphe.\"\"\"\n",
    "    P = set(G.nodes())\n",
    "    X = set()\n",
    "    R = set()\n",
    "    for v in sorted(G.nodes(), key=lambda v: G.degree(v), reverse=True):\n",
    "        bronKerbosch2(R.union({v}), P.intersection(set(G.neighbors(v))), X.intersection(set(G.neighbors(v))), G)\n",
    "        P.remove(v)\n",
    "        X.add(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.8703324794769287\n"
     ]
    }
   ],
   "source": [
    "G1 = nx.gnp_random_graph(100, 0.5, directed=False)\n",
    "\n",
    "# benchmark local\n",
    "R = set()\n",
    "P = set(G1.nodes())\n",
    "X = set()\n",
    "start = time.time()\n",
    "bronKerbosch1(R, P, X, G1)\n",
    "end = time.time()\n",
    "print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 19.248689651489258\n"
     ]
    }
   ],
   "source": [
    "# benchmark spark local\n",
    "start = time.time()\n",
    "parallel_bronKerbosch(sc, G1)\n",
    "end = time.time()\n",
    "print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark spark cluster\n",
    "!/opt/spark/bin/spark-submit --master spark://10.41.175.200:7077 --deploy-mode client --num-executors 4 --executor-cores 4 hard_test.py > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Results: {2, 98, 44, 19, 86, 57, 58, 93, 30}\n",
      "INFO:root:Time elapsed: 40.636481523513794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('spark_app.log', 'r') as log_file:\n",
    "    log_content = log_file.read()\n",
    "    print(log_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Task example on cluster](./cluster_task.png)  \n",
    "Pi calcultation task on cluster.  \n",
    "\n",
    "Our cluster is composed of 2 workers.  \n",
    "\n",
    "We can see that on some tasks using PySpark is faster (e.g. Pi calculation), but on others like bronKerbosch using the original implementation is way faster.  \n",
    "\n",
    "\n",
    "In any case our cluster is very slow to compute solutions. We noticed that it wasn't taking full potential of our 2 workers. And there may be an issue inside our code explaining these results (e.g. we may be using the wrong functions to parallelize our code).  \n",
    "\n",
    "We also noticed that the time to compute a solution is not linear with the number of workers. We think that this is due to the fact that we are using a shared memory.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our cluster should be expanded with more workers as we were not able to do so. The cluster should be faster to compute these solutions so we'll have to take a look at our implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
